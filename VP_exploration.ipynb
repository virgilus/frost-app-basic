{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a82e3d93",
   "metadata": {},
   "source": [
    "# Frost App Correction\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Aim\n",
    "\n",
    "The goal is to build a Streamlit app that will display the number of frost days for a given city in France computed over a decade. The data is provided by Météo France. It will eventually help winegrowers and farmers to make better decisions.\n",
    "\n",
    "For each day of the year, we want to know how many times the minimum temperature was below 0°C.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "This notebook will show you the process of finding the closest weather stations to each city in France. More than code, it will explain the reasoning behind each step. Building an app that uses data means also one must analyze and understand the data. If you skip this step and just copy/paste code from LLMs, many mistakes, sometimes hard to spot, will be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import. Each time we need a new library, we add it in this cell.\n",
    "\n",
    "# we want to reload all the imports each time we run a cell\n",
    "# so that we don't have to restart the kernel each time we change a library\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#from func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720ffa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants. Each time we need a new constant, we add it in this cell.\n",
    "# At the end we can all copy/paste them in a config.py file.\n",
    "RAW_DATA_PATH = \"data/raw\"\n",
    "PROCESSED_DATA_PATH = \"data/processed\"\n",
    "START_DATE = 2014\n",
    "END_DATE = 2023\n",
    "DEFAULT_WEATHER_FILENAME = \"Q_13_previous-1950-2023_RR-T-Vent.csv.gz\"\n",
    "COMPLETION_RATE_THRESHOLD = 0.65\n",
    "DEFAULT_WEATHER_URL = \"https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/QUOT/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddbef0c",
   "metadata": {},
   "source": [
    "## Weather data\n",
    "\n",
    "### Download the data\n",
    "\n",
    "You can read the data directly from the URL but when you build and test the app, it is better to have a local copy of the data. So we will download the data and save it in the `data/raw` folder. Our test file will be a small subset of the data, the department 13 (Bouches-du-Rhône).\n",
    "\n",
    "-> Download the data from (https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/QUOT/Q_13_previous-1950-2023_RR-T-Vent.csv.gz) and save it in `data/raw/Q_13_previous-1950-2023_RR-T-Vent.csv.gz`.\n",
    "\n",
    "Let's also add a constant DEFAULT_WEATHER_FILENAME so we can change the filename easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed8fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the directory if it does not exist\n",
    "os.makedirs(RAW_DATA_PATH, exist_ok=True)\n",
    "\n",
    "url = os.path.join(DEFAULT_WEATHER_URL, DEFAULT_WEATHER_FILENAME)\n",
    "\n",
    "# Download the file if it does not exist\n",
    "if not os.path.exists(os.path.join(RAW_DATA_PATH, DEFAULT_WEATHER_FILENAME)):\n",
    "    os.system(\n",
    "        f\"wget -O {os.path.join(RAW_DATA_PATH, DEFAULT_WEATHER_FILENAME)} {url}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c145a63",
   "metadata": {},
   "source": [
    "### GIT and data\n",
    "\n",
    "To avoid committing large files to git ( size superior to 10Mo), we will use a .gitignore file.\n",
    "If you don't have one already, create a file named `.gitignore` in the root of your project and add the following lines:\n",
    "```\n",
    "# Ignore data files\n",
    "data/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9751455",
   "metadata": {},
   "source": [
    "### Analysis of the weather data\n",
    "\n",
    "#### Data loading and overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9efbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv(\n",
    "    os.path.join(RAW_DATA_PATH, DEFAULT_WEATHER_FILENAME),\n",
    "    compression=\"gzip\",)\n",
    "\n",
    "weather_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9948292",
   "metadata": {},
   "source": [
    "The separator in the data is a semicolon `;` and not a comma `,`. So we need to specify that when reading the data with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05199598",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv(\n",
    "    os.path.join(RAW_DATA_PATH, DEFAULT_WEATHER_FILENAME),\n",
    "    compression=\"gzip\",\n",
    "    sep=';')\n",
    "\n",
    "weather_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d079b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699d45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a4e91e",
   "metadata": {},
   "source": [
    "#### Reading the documentation\n",
    "\n",
    "If you read the documentation (also called metadata) located [here](https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/QUOT/Q_descriptif_champs_RR-T-Vent.csv). You will see that the columns of interest are:\n",
    "\n",
    "```\n",
    "NUM_POSTE   : numéro Météo-France du poste sur 8 chiffres\n",
    "NOM_USUEL   : nom usuel du poste\n",
    "LAT         : latitude, négative au sud (en degrés et millionièmes de degré)\n",
    "LON         : longitude, négative à l’ouest de GREENWICH (en degrés et millionièmes de degré)\n",
    "ALTI        : altitude du pied de l'abri ou du pluviomètre si pas d'abri (en m)\n",
    "AAAAMMJJ    : date de la mesure (année mois jour)\n",
    "TN          : température minimale sous abri (en °C et 1/10)\n",
    "```\n",
    " But also maybe :\n",
    "\n",
    " ```\n",
    "TNSOL       : température quotidienne minimale à 10 cm au-dessus du sol (en °C et 1/10)\n",
    "TN50        : température quotidienne minimale à 50 cm au-dessus du sol (en °C et 1/10)\n",
    "```\n",
    "\n",
    "Let's reimport the data with the right columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1dafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'NUM_POSTE': (str, 'station_id'),\n",
    "    'NOM_USUEL': (str, 'station_name'),\n",
    "    'LAT': (float, 'latitude'),\n",
    "    'LON': (float, 'longitude'),\n",
    "    'ALTI': (float, 'alti'),\n",
    "    'AAAAMMJJ': (str, 'date'),\n",
    "    'TN': (float, 'tmin'),\n",
    "    'TNSOL': (float, 'tmin_ground'),\n",
    "    'TN50': (float, 'tmin_50cm'),\n",
    "}\n",
    "\n",
    "weather_df = pd.read_csv(os.path.join(RAW_DATA_PATH, DEFAULT_WEATHER_FILENAME),\n",
    "                           compression=\"gzip\",\n",
    "                           sep=';',\n",
    "                           usecols=d.keys(),\n",
    "                           dtype={k: v[0] for k, v in d.items()},\n",
    "                           ).rename(columns={k: v[1] for k, v in d.items()})\n",
    "\n",
    "# Let's parse the date column\n",
    "# We can technically do it in the read_csv function with the parse_dates argument\n",
    "# but we have a better control if we do it in a separate step\n",
    "\n",
    "weather_df['date'] = pd.to_datetime(weather_df['date'], format='%Y%m%d')\n",
    "\n",
    "weather_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363ecdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eb83e4",
   "metadata": {},
   "source": [
    "#### Temperatures analysis\n",
    "\n",
    "A lot of the values in the temperature columns seem empty. Let's see how many values are missing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a70e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42be196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataframe\n",
    "weather_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fe8247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for percentage of missing values in the dataframe\n",
    "weather_df.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20577396",
   "metadata": {},
   "source": [
    "#### Slice the dataframe to keep only the data from 2014 to 2023\n",
    "\n",
    "A lot of values are missing, but maybe it is only for the old data. Let's slice the dataframe to keep only the data from 2014 to 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f93b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing the dataframe to keep only the data from 2014 to 2023\n",
    "weather_df = weather_df.loc[weather_df['date'].dt.year.between(START_DATE, END_DATE, inclusive='both')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52caad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's check again for missing values in the dataframe\n",
    "weather_df.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7ede1",
   "metadata": {},
   "source": [
    "#### Assessing data quality\n",
    "\n",
    "The columns 'tmin_ground' and 'tmin_50cm' have a very high percentage of missing values. We will not use them in our analysis. The column 'tmin' has only   bout 25% of missing values, which is acceptable for our analysis.\n",
    "\n",
    "Let's check how the number of missing values evolves over the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad20ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "weather_df['tmin'].groupby(weather_df['date'].dt.year)\n",
    "                  .apply(lambda x: x.isnull().mean() * 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar chart showing the percentage of missing values over time with the number of stations in activity each year\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "missing_values = (weather_df['tmin'].groupby(weather_df['date'].dt.year)\n",
    "                  .apply(lambda x: x.isnull()\n",
    "                         .mean() * 100))\n",
    "\n",
    "# Get the number of stations in activity each year\n",
    "stations_in_activity = weather_df['station_id'].groupby(weather_df['date'].dt.year).nunique()\n",
    "\n",
    "# Create a dataframe to hold the missing values and stations in activity\n",
    "missing_values_df = pd.DataFrame({'missing_values': missing_values, 'stations_in_activity': stations_in_activity})\n",
    "\n",
    "missing_values_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb51d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function that returns a dataframe showing the percentage of missing values over time with the number of stations in activity each year\n",
    "def compute_missing_values_over_time(df):\n",
    "    missing_values = (df['tmin'].groupby(df['date'].dt.year)\n",
    "                      .apply(lambda x: x.isnull()\n",
    "                             .mean() * 100))\n",
    "\n",
    "    # Get the number of stations in activity each year\n",
    "    stations_in_activity = df['station_id'].groupby(df['date'].dt.year).nunique()\n",
    "\n",
    "    # Create a dataframe to hold the missing values and stations in activity\n",
    "    missing_values_df = pd.DataFrame({'missing_values': missing_values,\n",
    "                                      'stations_in_activity': stations_in_activity})\n",
    "    \n",
    "    return missing_values_df\n",
    "\n",
    "compute_missing_values_over_time(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c1fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL : USE .agg instead of .apply\n",
    "\n",
    "#Create a function that returns a dataframe showing the percentage of missing values over time with the number of stations in activity each year\n",
    "def missing_values_over_time_with_agg(df):\n",
    "    missing_values_df = (df.groupby(df['date'].dt.year)\n",
    "                      .agg({'tmin': lambda x: x.isnull().mean() * 100,\n",
    "                           'station_id': 'nunique'})).rename(columns={'tmin': 'missing_values',\n",
    "                                                                 'station_id': 'stations_in_activity'})\n",
    "    return missing_values_df\n",
    "missing_values_over_time_with_agg(weather_df).equals(compute_missing_values_over_time(weather_df))\n",
    "\n",
    "# Once it's working and the function is copied in func.py, we can delete this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56dc57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=missing_values.index, y=missing_values.values)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Percentage of Missing Values by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Percentage of Missing Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a086a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add in func.py\n",
    "\n",
    "def plot_missing_values_and_stations(df):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=df.index, y=df['missing_values'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Percentage of Missing Values by Year')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Percentage of Missing Values')\n",
    "    plt.show()\n",
    "plot_missing_values_and_stations(missing_values_over_time_with_agg(weather_df))\n",
    "\n",
    "# and then delete this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511076d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the distribution of completion rate per station\n",
    "completion_rate = weather_df.groupby('station_id')['tmin'].apply(lambda x: x.notnull().mean() * 100)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(completion_rate, bins=100, kde=True)\n",
    "plt.title('Distribution of Completion Rate per Station')\n",
    "plt.xlabel('Completion Rate (%)')\n",
    "plt.ylabel('Number of Stations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6d567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make it a function to add in func.py\n",
    "def plot_completion_rate_distribution(df: pd.DataFrame):\n",
    "    completion_rate = df.groupby('station_id')['tmin'].apply(lambda x: x.notnull().mean() * 100)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(completion_rate, bins=100, kde=True)\n",
    "    plt.title('Distribution of Completion Rate per Station')\n",
    "    plt.xlabel('Completion Rate (%)')\n",
    "    plt.ylabel('Number of Stations')\n",
    "    plt.show()\n",
    "    \n",
    "plot_completion_rate_distribution(weather_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d379be",
   "metadata": {},
   "source": [
    "Now we have a better idea of the data quality, as far as the department 13 (Bouches-du-Rhône) is concerned. Let's say a completion rate over 65% is acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_rate = weather_df.groupby('station_id')['tmin'].apply(lambda x: x.notnull().mean()) # we remove the * 100 to keep it between 0 and 1\n",
    "valid_stations = completion_rate[completion_rate >= COMPLETION_RATE_THRESHOLD].index # COMPLETION_RATE_THRESHOLD is between 0 and 1\n",
    "weather_df = weather_df[weather_df['station_id'].isin(valid_stations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffcdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_values_and_stations(compute_missing_values_over_time(weather_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_completion_rate_distribution(weather_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab228de",
   "metadata": {},
   "source": [
    "### Custom functions\n",
    "\n",
    "Let's create a function that will open a file with pandas, set the right dtypes, slice the dataframe to keep only the data from the start and end date, and remove all observations from stations that have a completion rate lower than the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db85806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function that will open a file with pandas, set the right dtypes,\n",
    "# slice the dataframe to keep only the data from the start and end date,\n",
    "# and remove all observations from stations that have a completion rate lower than the threshold.\n",
    "def process_weather_data(\n",
    "                         dept: str,\n",
    "                         local_file: bool=False,\n",
    "                         start_date: str=START_DATE,\n",
    "                         end_date: str=END_DATE,\n",
    "                         completion_rate_threshold: float=COMPLETION_RATE_THRESHOLD,\n",
    "                         remove_stations_below_threshold: bool=True,\n",
    "                         raw_data_path: str=RAW_DATA_PATH,\n",
    "                         default_url: str=DEFAULT_WEATHER_URL,\n",
    "                         ):\n",
    "\n",
    "    filename = f\"Q_{dept}_previous-1950-2023_RR-T-Vent.csv.gz\"\n",
    "    if local_file:\n",
    "        weather_filename = os.path.join(raw_data_path, filename)\n",
    "    else:\n",
    "        weather_filename = f\"{default_url}{filename}\"\n",
    "\n",
    "    d = {\n",
    "        'NUM_POSTE': (str, 'station_id'),\n",
    "        'NOM_USUEL': (str, 'station_name'),\n",
    "        'LAT': (float, 'latitude'),\n",
    "        'LON': (float, 'longitude'),\n",
    "        'ALTI': (float, 'alti'),\n",
    "        'AAAAMMJJ': (str, 'date'),\n",
    "        'TN': (float, 'tmin'),\n",
    "    }\n",
    "\n",
    "    weather_df = pd.read_csv(weather_filename,\n",
    "                            compression=\"gzip\",\n",
    "                            sep=';',\n",
    "                            usecols=d.keys(),\n",
    "                            dtype={k: v[0] for k, v in d.items()},\n",
    "                            ).rename(columns={k: v[1] for k, v in d.items()})\n",
    "\n",
    "    weather_df['date'] = pd.to_datetime(weather_df['date'], format='%Y%m%d')\n",
    "    \n",
    "    # Slice the dataframe to keep only the data from start_date to end_date\n",
    "    weather_df = weather_df.loc[weather_df['date'].dt.year.between(start_date, end_date, inclusive='both')]\n",
    "\n",
    "    # Remove all observations from stations that have a completion rate lower than the threshold\n",
    "    if remove_stations_below_threshold:\n",
    "        completion_rate = weather_df.groupby('station_id')['tmin'].apply(lambda x: x.notnull().mean())\n",
    "        valid_stations = completion_rate[completion_rate >= completion_rate_threshold].index\n",
    "        weather_df = weather_df[weather_df['station_id'].isin(valid_stations)]\n",
    "\n",
    "    return weather_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6b6d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = process_weather_data(dept='04', remove_stations_below_threshold=True)\n",
    "plot_missing_values_and_stations(compute_missing_values_over_time(_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5290bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_completion_rate_distribution(_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d757901e",
   "metadata": {},
   "source": [
    "## Cities data\n",
    "\n",
    "### Download cities\n",
    "\n",
    "https://www.data.gouv.fr/datasets/communes-et-villes-de-france-en-csv-excel-json-parquet-et-feather/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02443dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the file in data/raw\n",
    "\n",
    "# Creates the directory if it does not exist\n",
    "os.makedirs(RAW_DATA_PATH, exist_ok=True)\n",
    "\n",
    "filename = 'communes-france-2025.csv.gz'\n",
    "url = \"https://www.data.gouv.fr/api/1/datasets/r/6989ed1a-8ffb-4ef9-b008-340327c99430\"\n",
    "\n",
    "# Download the file if it does not exist\n",
    "os.system(f\"wget -O {os.path.join(RAW_DATA_PATH, filename)} {url}\") # Should work on linux and macOS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca5fc9",
   "metadata": {},
   "source": [
    "### Loading and analysing cities data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642474a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the communes df\n",
    "d = {\n",
    "    \"code_insee\": [\"string\", \"insee_code\"],\n",
    "    \"nom_standard\": [\"string\", \"name\"],\n",
    "    \"dep_code\": [\"string\", \"dep_code\"],\n",
    "    \"dep_nom\": [\"string\", \"dep_name\"],\n",
    "    \"latitude_centre\": [\"float32\", \"lat\"],\n",
    "    \"longitude_centre\": [\"float32\", \"lon\"],\n",
    "}\n",
    "\n",
    "city_df = pd.read_csv(\n",
    "    \"data/raw/communes-france-2025.csv.gz\",\n",
    "    compression=\"gzip\",\n",
    "    usecols=d.keys(),\n",
    "    dtype={k: v[0] for k, v in d.items()},\n",
    ").rename(columns={k: v[1] for k, v in d.items()})#.dropna()\n",
    "\n",
    "city_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e7babb",
   "metadata": {},
   "source": [
    "### Department list\n",
    "\n",
    "But first let's create a list of all the departments in France. A department is identified by its code, which is a string. The number \"04\" must have a zero in front of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f5034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adds_zero_if_needed(x: int) -> str:\n",
    "    if x < 10: return '0' + str(x)\n",
    "    else: return str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bd4cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or with a lambda function\n",
    "lambda x: str(x) if x >= 10 else '0' + str(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1af99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_list = [adds_zero_if_needed(dept) for dept in range(1, 96)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f78fa44",
   "metadata": {},
   "source": [
    "### Missing values in city_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98af33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if you use dropna() ?\n",
    "#city_df[city_df['insee_code'].str.contains('13055')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2112ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the cities that have missing values\n",
    "missing_cities = city_df[city_df.isnull().any(axis=1)]\n",
    "missing_cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af873cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decimal degrees in the WGS84 coordinate system\n",
    "missing_cities_lat_lon = {\n",
    "    \"Marseille\": [43.295, 5.372],\n",
    "    \"Paris\": [48.866, 2.333],\n",
    "    \"Culey\": [48.755, 5.266],\n",
    "    \"Les Hauts-Talican\": [49.3436, 2.0193],\n",
    "    \"Lyon\": [45.75, 4.85],\n",
    "    \"Bihorel\": [49.4542, 1.1162],\n",
    "    \"Saint-Lucien\": [48.6480, 1.6229],\n",
    "    \"L'Oie\": [46.7982, -1.1302],\n",
    "    \"Sainte-Florence\": [46.7965, -1.1520],\n",
    "}\n",
    "# Modify the dataframe to fill in the missing lat/lon values\n",
    "for city, (lat, lon) in missing_cities_lat_lon.items():\n",
    "    city_df.loc[city_df[\"name\"] == city, \"lat\"] = lat\n",
    "    city_df.loc[city_df[\"name\"] == city, \"lon\"] = lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f787943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a function that does all the cleaning process with cities_city_df\n",
    "def process_cities_data(raw_data_path: str=RAW_DATA_PATH,\n",
    "                        filename: str='communes-france-2025.csv.gz',\n",
    "                        dept_list: list|None=None,):\n",
    "    d = {\n",
    "        \"code_insee\": [\"string\", \"insee_code\"],\n",
    "        \"nom_standard\": [\"string\", \"name\"],\n",
    "        \"dep_code\": [\"string\", \"dep_code\"],\n",
    "        \"dep_nom\": [\"string\", \"dep_name\"],\n",
    "        \"latitude_centre\": [\"float32\", \"lat\"],\n",
    "        \"longitude_centre\": [\"float32\", \"lon\"],\n",
    "    }\n",
    "\n",
    "    city_df = pd.read_csv(\n",
    "        os.path.join(raw_data_path, filename),\n",
    "        compression=\"gzip\",\n",
    "        usecols=d.keys(),\n",
    "        dtype={k: v[0] for k, v in d.items()},\n",
    "    ).rename(columns={k: v[1] for k, v in d.items()})#.dropna()\n",
    "\n",
    "    # decimal degrees in the WGS84 coordinate system\n",
    "    missing_cities_lat_lon = {\n",
    "        \"Marseille\": [43.295, 5.372],\n",
    "        \"Paris\": [48.866, 2.333],\n",
    "        \"Culey\": [48.755, 5.266],\n",
    "        \"Les Hauts-Talican\": [49.3436, 2.0193],\n",
    "        \"Lyon\": [45.75, 4.85],\n",
    "        \"Bihorel\": [49.4542, 1.1162],\n",
    "        \"Saint-Lucien\": [48.6480, 1.6229],\n",
    "        \"L'Oie\": [46.7982, -1.1302],\n",
    "        \"Sainte-Florence\": [46.7965, -1.1520],\n",
    "    }\n",
    "    # Modify the dataframe to fill in the missing lat/lon values\n",
    "    for city, (lat, lon) in missing_cities_lat_lon.items():\n",
    "        city_df.loc[city_df[\"name\"] == city, \"lat\"] = lat\n",
    "        city_df.loc[city_df[\"name\"] == city, \"lon\"] = lon\n",
    "        \n",
    "    if dept_list is not None:\n",
    "        # remove all cities where the insee code is not inside the dept_list (only the 2 first characters)\n",
    "        city_df = city_df[city_df['insee_code'].str[:2].isin(dept_list)]\n",
    "\n",
    "    return city_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556db002",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_df = process_cities_data(dept_list=dept_list)\n",
    "city_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85191b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = process_cities_data()\n",
    "_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6500a40c",
   "metadata": {},
   "source": [
    "## Get a list of \"good stations\" for the entire country\n",
    "\n",
    "### Problematic\n",
    "\n",
    "We need to find the closest weather stations for each city. But there are two issues to overcome:\n",
    "\n",
    "- We want a station that has a good completion rate.\n",
    "- The closest station might be located in another department. So we need to get a list of good stations for the entire country.\n",
    "\n",
    "### Custom function\n",
    "\n",
    "We have the latitude and longitude of each city and each station. So we need to create a file that will return a dataframe with all the good stations for a list of departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff2b043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add this function in func.py\n",
    "\n",
    "# dept_list = ['04', '13']\n",
    "def get_all_good_stations(dept_list):\n",
    "    dfs = []\n",
    "    for dept in dept_list:\n",
    "        df = process_weather_data(dept=dept, remove_stations_below_threshold=True)\n",
    "        dfs.append(df[['station_id', 'station_name', 'latitude', 'longitude', 'alti']].drop_duplicates())\n",
    "        print(f\"Done with dept N° {dept}\")\n",
    "    return pd.concat(dfs)\n",
    "\n",
    "good_stations_df = get_all_good_stations(dept_list).reset_index(drop=True)\n",
    "\n",
    "good_stations_df.to_csv('good_stations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3192077",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_stations_df = good_stations_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a4d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_stations_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb5ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_stations_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b37a198",
   "metadata": {},
   "source": [
    "### Different approaches\n",
    "\n",
    "A basic approach would be to compute the distance between each city and each good station, and then select the closest one. However, this would require a lot of computations and would be very slow, if not impossible, to run on a local machine. Indeed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074423fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we wanted to compute the distance between each city and each good station, we would have to compute:\n",
    "city_df.shape[0] * good_stations_df.shape[0]\n",
    "# which is way too much. We need to find a more efficient way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b5677",
   "metadata": {},
   "source": [
    "#### More efficient approach using KDTree\n",
    "\n",
    "KDTree is a data structure that allows for efficient nearest neighbor searches. We can use the `scipy.spatial.KDTree` class to build a KDTree from the good stations' coordinates, and then query it with the cities' coordinates to find the closest station for each city.\n",
    "\n",
    "Pro: it is very fast even for large datasets.\n",
    "Con: Not as accurate as other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75898e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 0.3s to run\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "# Prepare city and station coordinates as arrays\n",
    "city_coords = city_df[['lat', 'lon']].to_numpy()\n",
    "station_coords = good_stations_df[['latitude', 'longitude']].to_numpy()\n",
    "\n",
    "# Build a KDTree for station coordinates\n",
    "tree = cKDTree(station_coords)\n",
    "\n",
    "# For each city, find the index of the closest station\n",
    "distances, indices = tree.query(city_coords)\n",
    "\n",
    "# Add the closest station index and distance to city_df\n",
    "city_df['closest_station_idx_with_kdtree'] = indices\n",
    "city_df['closest_station_distance_km_with_kdtree'] = distances * 100\n",
    "\n",
    "# Optionally, add station info (e.g., station name or properties)\n",
    "city_df['closest_station_name_with_kdtree'] = good_stations_df.iloc[indices]['station_name'].values\n",
    "city_df['closest_station_NUM_POSTE_with_kdtree'] = good_stations_df.iloc[indices]['station_id'].values\n",
    "city_df['closest_station_alti_with_kdtree'] = good_stations_df.iloc[indices]['alti'].values\n",
    "\n",
    "city_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e27caa",
   "metadata": {},
   "source": [
    "#### More efficient approach using Haversine formula\n",
    "\n",
    "The Haversine formula calculates the distance between two points on the surface of a sphere given their latitude and longitude. We can use it to compute distances between cities and stations more efficiently than computing all pairwise distances.\n",
    "\n",
    "Pro: More accurate than KDTree.\n",
    "Con: Slower than KDTree. (About 700 times slower)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0de5ead0",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# About 3mn50s to run for 36k cities and 7.5k stations\n",
    "from haversine import haversine, Unit\n",
    "import numpy as np\n",
    "\n",
    "# Prepare city and station coordinates as lists of (lat, lon)\n",
    "city_coords = city_df[['lat', 'lon']].to_numpy()\n",
    "station_coords = np.array(list(station_df['geometry.coordinates']))\n",
    "\n",
    "closest_station_idx = []\n",
    "closest_station_distance = []\n",
    "\n",
    "for city in city_coords:\n",
    "    # Compute all distances from this city to all stations\n",
    "    distances = [haversine(city, station, unit=Unit.KILOMETERS) for station in station_coords]\n",
    "    min_idx = np.argmin(distances)\n",
    "    closest_station_idx.append(min_idx)\n",
    "    closest_station_distance.append(distances[min_idx])\n",
    "\n",
    "# Add the closest station index and distance to city_df\n",
    "city_df['closest_station_idx_with_haversine'] = closest_station_idx\n",
    "city_df['closest_station_distance_km_with_haversine'] = closest_station_distance\n",
    "\n",
    "# More info\n",
    "city_df['closest_station_name_with_haversine'] = station_df.iloc[closest_station_idx]['properties.NOM_USUEL'].values\n",
    "city_df['closest_station_NUM_POSTE_with_haversine'] = station_df.iloc[closest_station_idx]['properties.NUM_POSTE'].values\n",
    "city_df['closest_station_NUM_DEP_with_haversine'] = station_df.iloc[closest_station_idx]['properties.NUM_DEP'].values\n",
    "\n",
    "city_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59157143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# About 3mn50s to run for 36k cities and 7.5k stations\n",
    "from haversine import haversine, Unit\n",
    "import numpy as np\n",
    "\n",
    "# Prepare city and station coordinates as lists of (lat, lon)\n",
    "city_coords = city_df[['lat', 'lon']].to_numpy()\n",
    "station_coords = good_stations_df[['latitude', 'longitude']].to_numpy()\n",
    "\n",
    "closest_station_idx = []\n",
    "closest_station_distance = []\n",
    "\n",
    "for city in city_coords:\n",
    "    # Compute all distances from this city to all stations\n",
    "    distances = [haversine(city, station, unit=Unit.KILOMETERS) for station in station_coords]\n",
    "    min_idx = np.argmin(distances)\n",
    "    closest_station_idx.append(min_idx)\n",
    "    closest_station_distance.append(distances[min_idx])\n",
    "\n",
    "# Add the closest station index and distance to city_df\n",
    "city_df['closest_station_idx_with_haversine'] = closest_station_idx\n",
    "city_df['closest_station_distance_km_with_haversine'] = closest_station_distance\n",
    "\n",
    "# More info\n",
    "city_df['closest_station_name_with_haversine'] = good_stations_df.iloc[closest_station_idx]['station_name'].values\n",
    "city_df['closest_station_NUM_POSTE_with_haversine'] = good_stations_df.iloc[closest_station_idx]['station_id'].values\n",
    "city_df['closest_station_alti_with_haversine'] = good_stations_df.iloc[closest_station_idx]['alti'].values\n",
    "\n",
    "city_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040a3893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd2d5eb3",
   "metadata": {},
   "source": [
    "## Assessing which approach one is the best\n",
    "\n",
    "KDTree vs Haversine formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7757bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_df.loc[city_df[\"closest_station_idx_with_kdtree\"] != city_df[\"closest_station_idx_with_haversine\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c92385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a function that does all this process and add it in func.py\n",
    "def find_closest_stations(city_df: pd.DataFrame,\n",
    "                          stations_df: pd.DataFrame,\n",
    "                          method: str='kdtree',\n",
    "                          add_station_info: bool=True,\n",
    "                          ) -> pd.DataFrame:\n",
    "\n",
    "    # Prepare city and station coordinates as arrays\n",
    "    city_coords = city_df[['lat', 'lon']].to_numpy()\n",
    "    station_coords = stations_df[['latitude', 'longitude']].to_numpy()\n",
    "    \n",
    "    if method == 'kdtree':\n",
    "        # Build a KDTree for station coordinates\n",
    "        tree = cKDTree(station_coords)\n",
    "\n",
    "        # For each city, find the index of the closest station\n",
    "        distances, indices = tree.query(city_coords)\n",
    "\n",
    "        # Add the closest station index and distance to city_df\n",
    "        city_df['closest_station_idx'] = indices\n",
    "        city_df['closest_station_distance_km'] = distances * 100\n",
    "        \n",
    "    elif method == 'haversine':\n",
    "\n",
    "        closest_station_idx = []\n",
    "        closest_station_distance = []\n",
    "\n",
    "        for city in city_coords:\n",
    "            # Compute all distances from this city to all stations\n",
    "            distances = [haversine(city, station, unit=Unit.KILOMETERS) for station in station_coords]\n",
    "            min_idx = np.argmin(distances)\n",
    "            closest_station_idx.append(min_idx)\n",
    "            closest_station_distance.append(distances[min_idx])\n",
    "\n",
    "        # Add the closest station index and distance to city_df\n",
    "        city_df['closest_station_idx'] = closest_station_idx\n",
    "        city_df['closest_station_distance_km'] = closest_station_distance\n",
    "        \n",
    "    # Optionally, add station info (e.g., station name or properties)\n",
    "    if add_station_info:\n",
    "        city_df['closest_station_name'] = stations_df.iloc[indices]['station_name'].values\n",
    "        city_df['closest_station_NUM_POSTE'] = stations_df.iloc[indices]['station_id'].values\n",
    "        city_df['closest_station_alti'] = stations_df.iloc[indices]['alti'].values\n",
    "\n",
    "    return city_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6099f4c1",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f65ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_df.to_csv(\"city_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663b118a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export a dict with the columns names and their types\n",
    "column_types = {col: str(dtype) for col, dtype in city_df.dtypes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5e2346",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
